# STT Analytics Platform

**Phase 1:** Speech-to-Text Data Normalization & Core Metrics Dashboard

A deterministic, mathematically precise analytics platform for call transcript analysis. Built for local execution with no cloud dependencies.

---

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **Robust CSV Ingestion** â€“ Handles multiple encodings (UTF-8, CP1250), semicolon-delimited format
- **Precise Time Calculations** â€“ Sweep-line algorithm for exact speech/silence/overlap metrics
- **Speaker Analytics** â€“ Raw and apportioned speaking time, turn-taking, interruptions
- **Interactive Dashboard** â€“ Streamlit + Plotly visualizations with drill-down
- **PDF Export** â€“ Professional reports with KPIs, charts, and data tables
- **Quality Metrics** â€“ Validates timestamps, detects inconsistencies, flags anomalies

### ğŸ“Š Computed Metrics

**Call-Level:**
- Total duration, speech time, silence time, overlap time
- Utterance count, average length, word count distribution
- Turn-taking speed, interruption counts
- Gap statistics (median, P95, negative gaps)

**Speaker-Level:**
- Raw speaking time (with overlaps)
- Apportioned speaking time (fair overlap distribution)
- Turn count and average duration per turn
- Words per minute (WPM)
- Dialog balance (Gini coefficient)

**Quality:**
- Invalid timestamp ratio
- Unknown speaker ratio
- Empty segment ratio
- Metadata vs. computed timeline delta

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.10+** ([download](https://www.python.org/downloads/))
- **Windows OS** (for oneclick.bat; Linux/Mac: use shell equivalent)

### Installation & Execution

1. **Clone or extract** this repository
2. **Place CSV files** in `data/raw/`
3. **Double-click** `oneclick.bat`

That's it! The script will:
- Create virtual environment
- Install dependencies
- Ingest and clean data
- Compute metrics
- Launch dashboard in browser

---

## ğŸ“ Project Structure

```
windsurf-project/
â”œâ”€â”€ stta/                          # Python package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                     # Typer CLI entry point
â”‚   â”œâ”€â”€ config/                    # Configuration files
â”‚   â”‚   â”œâ”€â”€ default.yml            # Default settings
â”‚   â”‚   â””â”€â”€ speakers.yml           # Speaker label mapping
â”‚   â”œâ”€â”€ io/                        # Data ingestion
â”‚   â”‚   â”œâ”€â”€ reader.py              # Robust CSV reader
â”‚   â”‚   â””â”€â”€ writer.py              # Parquet writer
â”‚   â”œâ”€â”€ schemas/                   # Pandera validation schemas
â”‚   â”‚   â”œâ”€â”€ calls.py
â”‚   â”‚   â””â”€â”€ utterances.py
â”‚   â”œâ”€â”€ metrics/                   # Core metric computations
â”‚   â”‚   â”œâ”€â”€ timeline.py            # Sweep-line algorithm
â”‚   â”‚   â”œâ”€â”€ call_level.py
â”‚   â”‚   â”œâ”€â”€ speaker_level.py
â”‚   â”‚   â”œâ”€â”€ quality.py
â”‚   â”‚   â””â”€â”€ registry.py            # Metric plugin system
â”‚   â”œâ”€â”€ dashboard/                 # Streamlit UI
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ components.py
â”‚   â”œâ”€â”€ report/                    # PDF generation
â”‚   â”‚   â””â”€â”€ pdf.py
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â”œâ”€â”€ timecode.py            # HH:MM:SS.mmm parser
â”‚       â”œâ”€â”€ text.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ validation.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Original CSV files (place here)
â”‚   â””â”€â”€ clean/                     # Parquet output
â”‚       â”œâ”€â”€ calls.parquet
â”‚       â”œâ”€â”€ utterances.parquet
â”‚       â””â”€â”€ *_metrics.parquet
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ reports/                   # Generated PDF reports
â”‚   â”œâ”€â”€ charts/                    # Exported chart images
â”‚   â””â”€â”€ run.log                    # Structured logs
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ data/                      # Golden test files
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ docs/                          # Extended documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ oneclick.bat                   # One-click launcher
â”œâ”€â”€ PROGRESS.md                    # Development tracker
â”œâ”€â”€ CHANGELOG.md                   # Version history
â””â”€â”€ README.md                      # This file
```

---

## ğŸ“– Usage

### Command-Line Interface

After activating the virtual environment:

```bash
# Ingest CSV files
python -m stta.cli ingest --input data/raw --output data/clean

# Compute metrics
python -m stta.cli compute --data data/clean

# Launch dashboard
streamlit run stta/dashboard/app.py -- --data data/clean

# Export PDF report for specific call
python -m stta.cli export-report --call-id CALL_001 --output artifacts/reports/
```

### Dashboard Features

- **Filters:** Call selection, speaker filter, time range
- **KPI Cards:** Duration, speech/silence ratio, overlap, turn-taking speed
- **Timeline (Gantt):** Visual representation of speaker turns and overlaps
- **Charts:** Speaking time distribution, gaps histogram, quality flags
- **Detail Table:** Sortable, filterable utterance list with download
- **Export:** Generate PDF report for selected call

---

## ğŸ”§ Configuration

### `config/default.yml`

```yaml
paths:
  raw_dir: "data/raw"
  clean_dir: "data/clean"
  artifacts_dir: "artifacts"

parsing:
  delimiter: ";"
  encodings_try: ["utf-8-sig", "cp1250", "latin-1"]
  time_fields: ["start_time", "end_time"]
  text_field: "text"
  speaker_field: "speaker"
  call_id_field: "call_id"

validation:
  require_call_id: true

dashboard:
  default_view: "call"
```

### `config/speakers.yml`

Map raw speaker labels to standardized categories:

```yaml
map:
  "Agent": "AGENT"
  "A": "AGENT"
  "OperÃ¡tor": "AGENT"
  "Customer": "CUSTOMER"
  "B": "CUSTOMER"
  "ZÃ¡kaznÃ­k": "CUSTOMER"
default: "UNKNOWN"
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=stta --cov-report=html

# Run specific test file
pytest tests/test_timeline.py

# Run property-based tests (Hypothesis)
pytest tests/test_sweepline_properties.py
```

### Golden Test Data

Located in `tests/data/`, these are minimal CSV files with known expected outputs for regression testing.

---

## ğŸ“ Mathematical Precision

### Sweep-Line Algorithm

All time-based metrics use a deterministic **sweep-line algorithm** to compute:

- **Union of intervals** (total speech time L)
- **Intersection of intervals** (overlap time O)
- **Apportioned speaking time** (fair distribution: A_k)
- **Silence time** (S = T - L)

**Invariants tested:**
- `L + S = T` (total timeline)
- `O â‰¤ L` (overlap subset of speech)
- `Î£ A_k = L` (apportioned sum equals total)

No approximations. No magic numbers.

---

## ğŸ› ï¸ Troubleshooting

### "No module named 'stta'"
- Ensure virtual environment is activated: `.venv\Scripts\activate`
- Reinstall: `pip install -r requirements.txt`

### "No CSV files found"
- Place CSV files in `data/raw/`
- Check file extension is `.csv`

### Encoding errors
- Reader tries UTF-8-sig â†’ CP1250 â†’ Latin-1 in order
- Check `artifacts/run.log` for details

### Dashboard won't start
- Check port 8501 is not in use
- Try: `streamlit run stta/dashboard/app.py --server.port 8502`

---

## ğŸ—ºï¸ Roadmap

### Phase 1 (Current)
- âœ… Core data pipeline
- âœ… Precise time metrics
- âœ… Interactive dashboard
- âœ… PDF export

### Phase 2 (Planned)
- Sentiment analysis (local lexicon-based)
- Topic modeling (sklearn LDA/NMF)
- Knowledge graph (entity extraction)
- WER benchmarking (when reference transcripts available)
- API ingestion adapters

---

## ğŸ“„ License

Proprietary. Internal use only.

---

## ğŸ‘¥ Contact

For questions or issues, contact the development team.

---

## ğŸ“š Documentation

Extended documentation available in `docs/`:
- Architecture deep-dive
- Metric formulas and derivations
- API integration guide (Phase 2)
- Performance optimization guide

---

**Built with:** Python, Pandas, Pyarrow, Pandera, Streamlit, Plotly, ReportLab  
**Philosophy:** Deterministic, auditable, mathematically precise analytics
